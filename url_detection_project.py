# -*- coding: utf-8 -*-
"""URL DETECTION PROJECT

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Cqoo0GFBVyrhBQeNrkomvcX8tbAessFC
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import io
from google.colab import files

files = files.upload()

data = pd.read_csv(io.BytesIO(files['dataset.csv']))

data.head()

"""**EXPLORATORY DATA ANALYSIS**"""

data.drop('URL',axis=1, inplace=True)

data.dropna(inplace=True)

filtered_df = data.drop(['CHARSET', 'SERVER', 'WHOIS_COUNTRY', 'WHOIS_STATEPRO','WHOIS_REGDATE', 'WHOIS_UPDATED_DATE' ], axis="columns")

filtered_df.columns

new_df = filtered_df

fig = plt.figure(figsize=(10,6))
sns.heatmap(new_df.corr(), annot=True)

"""**DATA VISUALIZATION**"""

plt.figure(figsize=(10, 10))
sns.barplot(data = new_df, orient='h')

sns.distplot(new_df['CONTENT_LENGTH'])

sns.distplot(new_df['Type'])

X = new_df.drop('Type', axis=1)
y = new_df['Type']

"""**SPLITTING THE DATASET**"""

from sklearn.model_selection import train_test_split

#splitting dataset into train and test sets
X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 1)

#splitting the train data into training set(80%) and validation set(20%)
X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=0)

X_train.shape

y = y.values.reshape(967,1)

models = list()
models.append(('knn', KNeighborsClassifier()))
models.append(('svm', SVC(probability=True)))
models.append(('bayes', GaussianNB()))
print(models)

# fit all models on the training set and predict on hold out set
meta_X = list()
for name, model in models:
	# fit in training set
	model.fit(X_train, y_train)
	# predict on hold out set
	yhat = model.predict(X_val)
	# reshape predictions into a matrix with one column
	yhat = yhat.reshape(len(yhat), 1)
	# store predictions as input for blending
	meta_X.append(yhat)

...
# create 2d array from predictions, each set is an input feature
meta_X = np.hstack(meta_X)

...
# define blending model
blender = RandomForestClassifier()
# fit on predictions from base models
blender.fit(meta_X, y_val)

# fit the blending ensemble
def fit_ensemble(models, X_train, X_val, y_train, y_val):
	# fit all models on the training set and predict on hold out set
	meta_X = list()
	for name, model in models:
		# fit in training set
		model.fit(X_train, y_train)
		# predict on hold out set
		yhat = model.predict(X_val)
		# reshape predictions into a matrix with one column
		yhat = yhat.reshape(len(yhat), 1)
		# store predictions as input for blending
		meta_X.append(yhat)
	# create 2d array from predictions, each set is an input feature
	meta_X = hstack(meta_X)
	# define blending model
	blender = RandomForestClassifier()
	# fit on predictions from base models
	blender.fit(meta_X, y_val)
	return blender

# make a prediction with the blending ensemble
def predict_ensemble(models, blender, X_test):
	# make predictions with base models
	meta_X = list()
	for name, model in models:
		# predict with base model
		yhat = model.predict(X_test)
		# reshape predictions into a matrix with one column
		yhat = yhat.reshape(len(yhat), 1)
		# store prediction
		meta_X.append(yhat)
	# create 2d array from predictions, each set is an input feature
	meta_X = hstack(meta_X)
	# predict
	return blender.predict(meta_X)

final_prediction =blender.predict(meta_X)

final_prediction

from sklearn.metrics import accuracy_score

X_test.shape

y_test.shape

final_score = accuracy_score(final_prediction, y_val)

final_score

